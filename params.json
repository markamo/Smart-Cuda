{"name":"Smart CUDA version 0.2.0 (initial release)","tagline":"Convenient CUDA wrappers for productive GPU programming","body":"![Smart CUDA](logo.png)\r\n# Welcome to Smart CUDA Library Project Page\r\n\r\n***\r\n\r\nSmart CUDA library is a lightweight C/C++ wrapper of the CUDA runtime API for productive natural-like CUDA programming. Smart CUDA follows the natural CUDA programming style; however, it provides low level abstraction to allow for a more convenient programming. In this way, Smart CUDA enhances developer productivity while attaining very high performance. Smart CUDA integrates seamlessly with C/C++ arrays and pointers, STL vector, and Thrust arrays. Smart CUDA only wraps the data on the device and host, therefore data on the host and device can have several wrappers and different views. This makes it very flexible, and allows easy integration with other platform and libraries.\r\n\r\n# Why Smart CUDA library? \r\nEven though I am relatively new to C/C++ and CUDA programming, I realized that a lightweight wrapper could boost gpu programming productivity. Thus, I developed several wrappers that preserved the natural programming style as I went through the CUDA Programming Guide. Smart CUDA is the compilation of the basic wrappers I have currently developed. Smart CUDA library is meant to complement the efforts of other libraries such as [Thrust](http://thrust.github.io/) and [VexCL](http://ddemidov.github.io/vexcl) and help boost gpu programming productivity.\r\n\r\n***\r\n\r\n# FEATURES\r\n\r\n## Header-only library\r\n```C++\r\n#include \"smartCuda_lib\\smartCuda.h\" ////include current version of smartCuda\r\n\r\n```\r\n## Minimal Learning\r\n**Only two things to learn: Smart Array and Smart Array Wrapper.** The Smart Array is for data allocation and Smart Array Wrapper is for management of allocated data.\r\n\r\n```C++\r\n//// memory and data allocation \r\n//// smartArray has overloads for allocating up to 4D data\r\ntemplate <typename T, int mem_loc> \r\n\tinline T* smartArray(int sizeX);\r\n\t\r\ntemplate <typename T, int mem_loc> \r\n\tinline T* smartArray( int sizeX, cudaError_t &cudaStatus);\r\n\r\n//// smartArray wrapper wraps arrays on cpu and gpu memories for convenient access and management\r\ntemplate <typename T, int mem_loc> \r\n\tclass smartArrayWrapper{};\r\n```\r\n***\r\n# Smart Array\r\n## Easy Memory Allocation\r\n```C++\r\ncudaError_t cudaStatus = cudaSuccess;\r\nconst int arraySize = 100000;\r\nint* h_a = smartArray<int,smartHost>(arraySize); ////pageable host memory\r\nint* hp_a = smartArray<int,smartPinnedHost>(arraySize); ////pinned host memory\r\nint* d_a = smartArray<int,smartDevice>(arraySize); ////device memory \r\n```\r\n\r\n## Multidimensional array allocation (up to 4D)\r\n```C++\r\nconst int lenX = 10;\r\nconst int lenY = 20;\r\nconst int lenZ = 5;\r\nconst int lenW = 3;\r\n\r\n////allocation on CPU\r\nint* h_1D = smartArray<int,smartHost>(lenX);\r\nint* h_2D = smartArray<int,smartHost>(lenX, lenY);\r\nint* h_3D = smartArray<int,smartHost>(lenX, lenY, lenZ);\r\nint* h_4D = smartArray<int,smartHost>(lenX, lenY, lenZ, lenW);\r\n\r\n////allocation on GPU\r\nint* d_1D = smartArray<int,smartDevice>(lenX);\r\nint* d_2D = smartArray<int,smartDevice>(lenX, lenY);\r\nint* d_3D = smartArray<int,smartDevice>(lenX, lenY, lenZ);\r\nint* d_4D = smartArray<int,smartDevice>(lenX, lenY, lenZ, lenW);\r\n\r\n////use of inline array to allocate memory in kernels\r\nint* arr = smartArray<int,smartInlineArray>(arr_size); //create an inline array of size arr_size;\r\n...\r\n__global__ void testKernel(int arr_size)\r\n{\r\n    int* arr = smartArray<int,smartInlineArray>(arr_size); //create an inline array of size arr_size;\r\n    ...\r\n   smartInlineArrayFree(arr);\r\n\r\n}\r\n...\r\n```\r\n***\r\n\r\n# Smart Array Wrapper\r\n## Convenient wrapper for data management\r\n```C++\r\n...\r\n////wrap array on host\r\nsmartArrayWrapper<int,smartHost> wHa(h_a,arraySize,scopeLocal);  \r\nsmartArrayWrapper<int,smartHost> wHb(h_b,arraySize,scopeLocal);\r\nsmartArrayWrapper<int,smartHost> wHc(h_c,arraySize,scopeLocal);\r\n\r\n////wrap array on device\r\nsmartArrayWrapper<int,smartDevice> wDa(d_a,arraySize,scopeLocal);\r\nsmartArrayWrapper<int,smartDevice> wDb(d_b,arraySize,scopeLocal);\r\nsmartArrayWrapper<int,smartDevice> wDc(d_c,arraySize,scopeLocal);\r\n\r\n/////alternative wrap method\r\nsmartArrayWrapper<int,smartHost> wHa1;\r\nwHa1.wrap(h_a,arraySize,scopeGlobal);\r\n...\r\n\r\n////access the underlying array using the object.inner_ptr()\r\n\r\n......\r\n__global__ void addKernel(int *c, const int *a, const int *b)\r\n{\r\n\tint i = threadIdx.x + blockIdx.x * blockDim.x;\r\n    c[i] = a[i] + b[i];\r\n\ti += blockDim.x * gridDim.x;\r\n}\r\n\r\n....\r\n////access the underlying array using the object.inner_ptr()\r\naddKernel<<<16, 16>>>(wDc.inner_ptr(), wDa.inner_ptr(), wDb.inner_ptr());\r\n\r\n```\r\n\r\n\r\n## Seamless data transfers between CPU and GPU\r\n```C++\r\n...\r\n////wrap array on host\r\nsmartArrayWrapper<int,smartHost> wHa(h_a,arraySize,scopeLocal);  \r\nsmartArrayWrapper<int,smartHost> wHb(h_b,arraySize,scopeLocal);\r\nsmartArrayWrapper<int,smartHost> wHc(h_c,arraySize,scopeLocal);\r\n\r\n////wrap array on device\r\nsmartArrayWrapper<int,smartDevice> wDa(d_a,arraySize,scopeLocal);\r\nsmartArrayWrapper<int,smartDevice> wDb(d_b,arraySize,scopeLocal);\r\nsmartArrayWrapper<int,smartDevice> wDc(d_c,arraySize,scopeLocal);\r\n\r\n////transfer data from host to device \r\nwDa = wHa; ////quick data transfer\r\n\r\n////copy method has extended support and overloads for other data types\r\nwDb.copy(wHb); \r\n...\r\nwDa.copy(wHa.inner_ptr(),wHa.getlen(),wHa.getType());\r\nwDb.copy(wHb.inner_ptr(),wHb.getlen(),wHb.getType());\r\n....\r\n////do some work on the GPU\r\n\r\n\r\n////transfer data back to CPU\r\nwHc = wDc; \r\n\r\n```\r\n\r\n## Local and Global scopes for automatic memory deallocation\r\n**scopeLocal**: for Local scopes. \r\n**scopeGlobal**: to persist data beyond the current scope. \r\n**__clean_globals = ON/OFF**: to clean global wrapper data declared in current scope. \r\n```C++\r\n...\r\nconst int arraySize = 1000;\r\nint* h_1 = smartArray<int,smartHost>(arraySize);\r\nint* h_2 = smartArray<int,smartHost>(arraySize);\r\n{\r\n    ////wrap host and device data\r\n    ////use scopeLocal for managing data that is for local scopes\r\n    ////memory is automatically freed when the wrapper goes out of scope    \r\n    smartArrayWrapper<int,smartHost> wLocal(h_1,arraySize,scopeLocal); \r\n\r\n    ...\r\n    ////use scopeGlobal for managing data that is for global scopes\r\n    smartArrayWrapper<int,smartHost> wGlobal(h_2,arraySize,scopeGlobal);\r\n    ....\r\n    /////do some work\r\n    ....\r\n}//// data for h_1 automatically deleted because it is local scope. \r\n//// data for h_2 is preserved because it is global to the scope created. \r\n\r\n//// h_1 must be reinitialized before used again\r\nh_1 = smartArray<int,smartHost>(arraySize);\r\n\r\nsmartArrayWrapper<int,smartHost> w2(h_1,arraySize,scopeGlobal); \r\n////do some work\r\n...\r\n////manual deletion of allocated memory\r\nw2.destroy();\r\n\r\n....\r\n////use __clean_globals = ON/OFF to clean global scope wrappers when they get out of their current scope\r\n...\r\nError:\r\n    cudaFree(dev_c);\r\n    cudaFree(dev_a);\r\n    cudaFree(dev_b);\r\n    \r\n    return cudaStatus;\r\n\r\n///// can be replaced by \r\n...\r\nError:\r\n    __clean_globals = ON; //// delete global wrappers and their underlying data\r\n   //__clean_globals = OFF; //// preserve the data that the global wrappers are pointing to\r\n\r\n    return cudaStatus;\r\n```\r\n\r\n\r\n## Aliases for Thread Indexing  \r\n\r\n```C++\r\n//// alias for __global__ \r\n#define __KERNEL__ __global__ \r\n\r\n////indexing along one dimension x,y,z = 0,1,2. default index is along the x dimension (0)\r\n // alias for threadIdx.x;\r\n__device__ inline int __local_index(int dim = 0);\r\n\r\n // alias for blockIdx.x * blockDim.x + threadIdx.x;\r\n__device__ inline int __global_index(int dim = 0);\r\n\r\n // alias for blockIdx.x;\r\n__device__ inline int __block_index(int dim = 0);\r\n__device__ inline int __group_index(int dim = 0);\r\n\r\n // alias for gridDim.x * blockDim.x;\r\n__device__ inline int __stride(int dim = 0);\r\n\r\n////alias functions for size of threads and blocks along the x, y, z and total size in all dimensions (i.e. 0,1,2,-1). The default is the total size along all dimensions (-1).\r\n\r\n // alias for gridDim.x;\r\n__device__ inline int __num_blocks(int dim = -1 );\r\n__device__ inline int __num_groups(int dim = -1);\r\n\r\n // alias for gridDim.x;\r\n__device__ inline int __block_size(int dim = -1);\r\n__device__ inline int __group_size(int dim = -1);\r\n__device__ inline int __local_size(int dim = -1);\r\n\r\n // alias for gridDim.x * blockDim.x;\r\n__device__ inline int __launch_size(int dim = -1);\r\n__device__ inline int __global_size(int dim = -1);\r\n\r\n``` \r\n\r\n\r\n## Indexing and Data Access (up to 4D) \r\n```C++\r\n...\r\nconst int lenX = 100;\r\nconst int lenY = 100;\r\n\r\nsmartArrayWrapper<int,smartDevice> wA(d_a,lenX, lenY,scopeLocal);\r\nsmartArrayWrapper<int,smartDevice> wB(d_b,lenX, lenY,scopeLocal);\r\nsmartArrayWrapper<int,smartDevice> wC(d_c,lenX, lenY,scopeLocal);\r\n\r\nint i = threadIdx.x + blockIdx.x * blockDim.x;\r\nint j = threadIdx.y + blockIdx.y * blockDim.y;\r\n\r\n////the following gives the same results\r\n//// traditional CUDA style\r\n////d_c[i + j * lenX] = d_a[i + j * lenX] * d_b[i + j * lenX]; \r\n//// traditional CUDA style possible with smart array wrapper\r\n////wC[i + j * lenX] = wA[i + j * lenX] * wB[i + j * lenX]; \r\n////wC(i + j * lenX) = wA(i + j * lenX) * wB(i + j * lenX); \r\n////convenient indexing\r\nwC(i,j) = wA(i,j) * wB(i,j); \r\n\r\n////supports upto 4 dimensions \r\nsmartArrayWrapper<int,smartDevice> w4(d_a,lenX, lenY, lenZ, lenW, scopeLocal);\r\nint results = w4(5,5,5,5);\r\n\r\n////access the nth data element \r\nint results = w4[117];\r\n\r\n```\r\n\r\n\r\n## Customized Views of Pre-Allocated Data (up to 4D) \r\n```C++\r\n...\r\nconst int arraySize = 1000;\r\nint* h_1 = smartArray<int,smartHost>(arraySize);\r\n...\r\n////wrap array on host\r\nsmartArrayWrapper<int,smartHost> wView2D(h_1,arraySize,scopeLocal);  \r\nwView2D.setViewDim(100,10); //// view 1D array as a 2D 100 * 10 array\r\nint temp = wView2D.ViewAt(50,5); //// get data at 50,10 of the view (up to 4D view)\r\nint temp2 = wView2D.ViewAt_2D(45,2); //// alternative view method (only 2D view)\r\n\r\nsmartArrayWrapper<int,smartHost> wView3D(h_1,arraySize,scopeLocal);\r\nwView3D.setViewDim(10,10,10); //// view 1D array as a 3D 10 * 10 * 10 array\r\nint temp3 = wView3D.ViewAt(5,5,5); //// get data at 5,5,5 of the view (up to 4D view)\r\nint temp4 = wView3D.ViewAt_3D(4,5,2); //// alternative view method (only 3D view)\r\n\r\n////access the original array \r\n//// the setViewDim and viewAt methods do not modify the layout of the array\r\nint temp5 = wWiew3D[150]; ////access the original h_1 array at index 150 using []\r\nint temp6 = wWiew2D(300); ////access the original h_1 array at index 300 using ()\r\n...\r\n```\r\n\r\n## Navigation (up to 4D indexing) \r\n**Navigation using PEEK, SEEK, and ADV(advance)**\r\n```C++\r\n...\r\nconst int arraySize = 1000;\r\nint* d_1 = smartArray<int,smartDevice>(arraySize);\r\n...\r\n////wrap array on device\r\nsmartArrayWrapper<int,smartDevice> wNav(d_1,arraySize,scopeLocal);  \r\n...\r\n////navigate through the data \r\n////peek examples\r\nint peek1 = wNav.peek(); ////get the next data without moving the data access position\r\nint peek5 = wNav.peek(5); ////get the next 5th position data without moving the data access position\r\nint peek_3 = wNav.peek(-3); ////get the previous 3rd position data without moving the data access position\r\n...\r\n////adv examples\r\nint adv1= wNav.seek(); ////move to the next data and return a reference to the data\r\nint adv5 = wNav.seek(5); ////move to the next 5th position data and return a reference\r\nint adv_3 = wNav.seek(-3); ////move to the previous 3rd position data and return a reference\r\n\r\n...\r\n////seek examples\r\nint seek1 = wNav.seek(); ////move to the next data and return a reference to the data\r\nint seek5 = wNav.seek(5); ////move to the 5th position data and return a reference\r\nint seek_3 = wNav.seek(3); ////move to the 3rd position data and return a reference ////negative index not allow\r\n...\r\n\r\n```\r\n\r\n**Navigation using PEEK4, SEEK4, and ADV4(advance4)**\r\n```C++\r\n....\r\n/////other peek, seek and adv methods\r\n////peek4(),seek4(),adv4() uses convenient indexing to navigate the data as above\r\nint seek4 = wNav.seek4(2,5,1,3); ////move to the 2,5,1,3 position data and return a reference\r\nint adv4 = wNav.adv4(-3,1,2); ////move to -3,1,2 position from the current index data and return a reference\r\nint peek4 = wNav.peek4(5,7); ////get the data at index 5,7 from the current position without moving the data access position\r\n...\r\n\r\n```\r\n\r\n**Navigation on Customized Views VPEEK(), PEEK4(), VSEEK(), VSEEK4(), VADV(), and VADV4()**\r\n```C++\r\n....\r\n/////navigation on customized views: vpeek(), vseek() and vadv() methods\r\n////vpeek4(),vseek4(),vadv4() uses convenient indexing to navigate customized views\r\n\r\n...\r\ncvNav.setViewDim(10,10,...,...); //// set a customized array view \r\n\r\n...\r\n/////navigating customized views\r\nint cseek = cvNav.vseek(); ////move to the next data and return a reference to the data\r\nint cadv = cvNav.vadv(8); ////move to the 8th position data and return a reference\r\nint cpeek = cvNav.vpeek(3); ////return data value at position 3 from current location\r\n\r\n...\r\n////multidimensional navigation of customized views\r\nint cvseek4 = cvNav.vseek4(2,1,4,6); ////move to the 2,1,4,6 of the customized array view\r\nint cvadv4 = cvNav.vadv4(4,1,2); ////move to 4,1,2 position from the current index data and return a reference\r\nint cvpeek4 = cvNav.vpeek4(3,3); ////get the data at index 3,3 from the current position from the customized view without moving the data access position\r\n...\r\n\r\n```\r\n\r\n**Navigation using ++ and --**\r\n```C++\r\n....\r\n/////other navigation methods\r\nint nav_ref = wNav++; ////returns a reference to the next data element and increases the data access index\r\nint nav = ++wNav; ////returns the value of the next data element and increases the data access index\r\n\r\nint nav_ref1 = wNav--; ////returns a reference to the previous data element and decreases the data access index\r\nint nav1 = --wNav; ////returns the value of the previous data element and decreases the data access index\r\n\r\n\r\n```\r\n***\r\n# Other Features\r\n## Smart Device Management \r\n```C++\r\nsmartDeviceManager dev;\r\ndev.mapDevice(0);\r\ndev.setDevice();\r\n....\r\ndev.resetDevice();\r\n```\r\n\r\n## Smart Event Timings\r\n```C++\r\nsmartEvent stopwatch;\r\n...\r\n\t\t\r\nstopwatch.recStart();\r\nfor (int i = 0; i < arraySize; i++)\r\n{\r\n\t//// printf(\"index %d: %d + %d = %d\\n\", i, wHa[i],wHb[i], wHc[i]);\r\n\tif (wHc(i) < 0)\r\n\t{\r\n\t\twHc(i) = wHa(i) + wHb(i);\r\n\t}\r\n}\r\nstopwatch.recStop();\r\nstopwatch.elapsedTime();\r\nstopwatch.printElapsedTime();\r\n\r\n\r\n```\r\n\r\n## Array Initializations\r\n```C++\r\n\r\nconst unsigned int arraySize = 10000000;\r\nint* h_a = smartArray<int,smartHost>(arraySize);\r\nint* hp_a = smartArray<int,smartPinnedHost>(arraySize);\r\nint* h_b = smartArray<int,smartHost>(arraySize);\r\nint* h_c = smartArray<int,smartHost>(arraySize);\r\n\r\n....\r\nidx_initializeSmartArray<int>(h_a,arraySize,0,1);\r\nidx_initializeSmartArray<int>(h_b,arraySize,-0,1);\r\ninitializeSmartArray<int>(h_c,arraySize,0);\r\nidx_initializeSmartArray<int>(hp_a,arraySize,0);\r\n...\r\ninitializeSmartArrayAsync<int>(d_c,arraySize,-100);\r\n```\r\n\r\n## Smart Kernel Configuration (Beta)\r\n```C++\r\nconst unsigned int arraySize = 10000000;\r\n....\r\nsmartConfig config(arraySize);\r\naddKernel<<<config.getBlockSize(), config.getThreadSize()>>>(wDc.inner_ptr(), wDa.inner_ptr(), wDb.inner_ptr(), arraySize);\r\n\r\n```\r\n\r\n## Smart Random Numbers and Transformations\r\n* New Kernel function ``` appy_func_core```, perform parallel element wise operations on allocated device arrays\r\n```C++ \r\ntemplate <typename T, class Op> __global__  \r\nvoid appy_func_core(T* dev_Array, const int size, Op fn);\r\n\r\ntemplate <typename T, class Op> __global__\r\n void appy_func_core(T* dest, T* src, const int size, Op fn);\r\n\r\ntemplate <typename T, class Op> __global__\r\n void appy_func_core(T* dest, T* src1, T* src2, const int size, Op fn);\r\n\r\n``` \r\n\r\n* New Kernel function ``` transform_core```, perform parallel element wise transformations on allocated device arrays. Supports up to 10 allocated device arrays\r\n```C++\r\ntemplate <typename T, class Op> __global__\r\n void transfrom_core(T* arr, const int size, Op fn );\r\n\r\ntemplate <typename T, class Op> __global__\r\n void transfrom_core(T* arr, T* arr1, T* arr2, const int size, Op fn );\r\n\r\n...\r\n\r\ntemplate <typename T, class Op> __global__\r\n void transfrom_core(T* arr, T* arr1, T* arr2, T* arr3, T* arr4,  T* arr5,  T* arr6, T* arr7, T* arr8, T* arr9, const int size, Op fn );\r\n\r\n...\r\n\r\n``` \r\n\r\n* New Kernel function ``` transform_core_t```, perform parallel element wise transformations on allocated device arrays of different types. Supports up to 10 allocated device arrays and types\r\n```C++ \r\ntemplate <typename T, class Op> __global__\r\n void transfrom_core_t(T* arr, const int size, Op fn );\r\n\r\n\r\ntemplate <typename T, typename T1, typename T2, class Op> __global__\r\n void transfrom_core_t(T* arr, T1* arr1, T2* arr2, const int size, Op fn );\r\n\r\n...\r\n\r\ntemplate <typename T, typename T1, typename T2,  typename T3, typename T4, typename T5, typename T6, typename T7, typename T8, typename T9, class Op> __global__\r\n void transfrom_core_t(T* arr, T1* arr1, T2* arr2, T3* arr3, T4* arr4,  T5* arr5,  T6* arr6, T7* arr7, T8* arr8, T9* arr9, const int size, Op fn );\r\n\r\n...\r\n\r\n``` \r\n\r\n* Simple Reduction function and function operators for reductions on gpu. Customer implementation of smartOperator can be used with for the reduction kernel.\r\n\r\n```C++ \r\n////pre-defined operators\r\ntemplate <typename T> class smartOperator;\r\ntemplate <typename T> class smartPlus: public smartOperator<T>;\r\ntemplate <typename T> class smartMultiply: public smartOperator<T>;\r\ntemplate <typename T> class smartMax: public smartOperator<T>;\r\ntemplate <typename T> class smartMin: public smartOperator<T>;\r\ntemplate <typename T> class smartOR: public smartOperator<T>;\r\ntemplate <typename T> class smartAND: public smartOperator<T>;\r\n\r\n////kernel launcher\r\ntemplate<typename T, class Op>\r\nvoid smartReduce_kl(T *answer, T *partial, const T *in, size_t N, int numBlocks, int numThreads, Op fn );\r\n\r\n\r\n``` \r\n\r\n* Smart Random library for random number generation in on device kernels. Use of a default random number that can be called from any part of the code. Smart Random library provides a lightweight wrapper on cuRand library.\r\n\r\nFunctions:\r\n```C++ \r\n __device__ curandState *defaultStates;\r\n...\r\n__host__ cudaError_t initDefaultRand(int size = 64 * 64, int seed = time(NULL));\r\n__host__ cudaError_t releaseDefaultRand();\r\n\r\n``` \r\n\r\nUsage:\r\n```C++ \r\ninitDefaultRand(256*256);\r\n....\r\n////use defaultStates in cuRand calls\r\n....\r\n\r\nreleaseDefaultRand(); //// called when done using defaultStates to release memory allocated;\r\n\r\n``` \r\n\r\n\r\n* Other Smart Random library functions and kernels\r\n```C++ \r\n__global__ void setup_rand_kernel(curandState *state, unsigned int size, unsigned int seed);\r\n\r\ntemplate <typename T>\r\n__global__ void generate_uniform_kernel(T* result, int size, curandState *state );\r\n\r\ntemplate <typename T>\r\n__global__ void generate_uniform_range_kernel(T* result, T lower, T upper, int size, curandState *state );\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandu(T *dev_Array, const int size, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandu(T *dev_Array, const int sizeX, const int sizeY, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandu(T *dev_Array, const int sizeX, const int sizeY, const int sizeZ, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandu(T *dev_Array, const int sizeX, const int sizeY, const int sizeZ, const int sizeW, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandr(T *dev_Array, T min, T max, const int size, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandr(T *dev_Array, T min, T max, const int sizeX, const int sizeY, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandr(T *dev_Array, T min, T max, const int sizeX, const int sizeY, const int sizeZ, curandState *state = defaultStates);\r\n\r\ntemplate <typename T> \r\n__host__ cudaError_t smartRandr(T *dev_Array, T min, T max, const int sizeX, const int sizeY, const int sizeZ, const int sizeW, curandState *state = defaultStates);\r\n\r\n\r\n``` \r\n\r\n***\r\n\r\n***\r\n\r\n# Latest News\r\n* [Smart CUDA v0.2.0 (Initial Release) - 18th January, 2014](https://github.com/markamo/Smart-Cuda/releases/tag/v0.2.0)\r\n* [Smart CUDA v0.1.2 (Initial Release) - 19th December, 2013](https://github.com/markamo/Smart-Cuda/releases/tag/v0.1.2)\r\n* [Smart CUDA v0.1.1 (Initial Release) -17th December, 2013](https://github.com/markamo/Smart-Cuda/releases/tag/v0.1.1)\r\n* [SmartCUDA v 0.0.1(draft-release) - 16th December, 2013](https://github.com/markamo/Smart-Cuda/releases/tag/v0.0.1d)\r\n\r\n\r\n### Features under consideration for future releases\r\n- [-] Smart Kernel\r\n- [-] Smart Device\r\n- [-] SmartArrayWrapper.apply_func()\r\n- [ ] SmartArrayWrapper.apply_funcAsync()\r\n- [ ] SmartArrayWrapper.sort()\r\n- [ ] SmartArrayWrapper.sortAsync()\r\n- [ ] SmartArrayWrapper.reduce()\r\n- [ ] SmartArrayWrapper.scan()\r\n- [ ] Smart Array Wrapper basic mathematical operators\r\n- [-] Full integration with STL::array and STL::vector\r\n- [-] Full integration with Thrust::host_vector and Thrust::device_vector\r\n- [ ] Basic integration with OpenCL, OpenMP, TBB, and C++ AMP \r\n- [ ] Integration with other CUDA libraries \r\n- [-] Multi-Host and Multi-Device data allocation and management\r\n- [ ] Etc.\r\n\r\n### Authors and Contributors\r\nThe original creator of Smart CUDA is Mark Amo-Boateng (@markamo). \r\n\r\n### Support or Contact\r\nHaving trouble with Smart CUDA? Check out this [Wiki] (https://github.com/markamo/Smart-Cuda/wiki). Visit http://markamo.github.io/Smart-Cuda/ or https://github.com/markamo/Smart-Cuda for latest news and source codes. Feel free to contact smartcuda@outlook.com for additional support.","google":"UA-46530332-1","note":"Don't delete this file! It's used internally to help with page regeneration."}